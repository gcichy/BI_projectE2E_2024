from langchain_postgres import PGVector
from langchain_openai import OpenAIEmbeddings

CONN_STR = "postgresql+psycopg://user:pass@host:5432/dbname"

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

vectorstore = PGVector(
    connection=CONN_STR,
    embeddings=embeddings,
    collection_name="my_collection",
    use_jsonb=True,  # common/default in newer setups
)

from langchain_core.documents import Document

def retrieve_relevant_docs(
    query: str,
    k: int = 6,
    min_score: float = 0.55,
    filter: dict | None = None,
) -> list[Document]:
    """
    Returns docs only if they meet min_score, otherwise [].
    """
    results = vectorstore.similarity_search_with_relevance_scores(
        query,
        k=k,
        filter=filter,      # optional metadata filter
    )
    # results: list[(Document, score)] where score is usually normalized to [0..1]
    good = [doc for doc, score in results if score >= min_score]
    return good

def retrieve_with_gap_gate(
    query: str,
    k: int = 6,
    min_score: float = 0.55,
    min_gap: float = 0.08,
    filter: dict | None = None,
) -> list[Document]:
    results = vectorstore.similarity_search_with_relevance_scores(
        query, k=k, filter=filter
    )
    if not results:
        return []

    # Sort high→low by score
    results = sorted(results, key=lambda x: x[1], reverse=True)
    top_doc, top_score = results[0]

    # If top is low, reject
    if top_score < min_score:
        return []

    # If top is not meaningfully better than runner-up, reject (optional)
    if len(results) > 1 and (top_score - results[1][1]) < min_gap:
        return []

    return [doc for doc, score in results if score >= min_score]

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

prompt = ChatPromptTemplate.from_messages([
    ("system",
     "You are a QA assistant. Answer ONLY using the provided context. "
     "If the context does not contain the answer, say you don't know."),
    ("human", "Question: {question}\n\nContext:\n{context}")
])

def answer_question(question: str, user_id: str | None = None) -> str:
    # Optional: ensure you only search the user’s uploaded docs
    metadata_filter = {"user_id": user_id} if user_id else None

    docs = retrieve_with_gap_gate(
        question,
        k=6,
        min_score=0.58,
        min_gap=0.08,
        filter=metadata_filter,
    )

    if not docs:
        return "I can’t find anything related to that in the uploaded documents."

    context = "\n\n".join(d.page_content for d in docs)
    return llm.invoke(
        prompt.format_messages(question=question, context=context)
    ).content

